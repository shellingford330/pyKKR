{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of (sub)KKR and (sub)KBR to an Analytical Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable inline plotting\n",
    "%matplotlib inline\n",
    "# uncomment this line to open a console with the same kernel\n",
    "%qtconsole\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.linalg as linalg\n",
    "import scipy.spatial as spatial\n",
    "import scipy.stats as stats\n",
    "\n",
    "#  RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# heatmap\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "output_notebook()\n",
    "\n",
    "from kkr import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of a Gaussian distribution\n",
    "\n",
    "In this notebook, we want to show a comparison of the (subspace) kernel Kalman rule ((sub)KKR) and the (subspace) kernel Bayes' rule ((sub)KBR) to an analytical baseline.\n",
    "The idea of this comparison is the following: we get samples from a Gaussian distribution, but don't know the mean.\n",
    "In the first part we assume the variance still to be known, later we want to infer the variance as well.\n",
    "\n",
    "Using a set of training samples, we can embed a prior distribution and learn the operators for (sub)KBR and (sub)KKR.\n",
    "We can use these then to iteratively infer the mean (and later the variance) of an unknow Gaussian distribution by only observing samples from it.\n",
    "To compare both inference methods to a baseline, we can also do this analytically by assuming a conjugate prior and white Gaussian noise.\n",
    "\n",
    "### Assuming the variance to be known\n",
    "\n",
    "Let's say we have a known and constant variance of the unknown Gaussian with variance $\\sigma^2 = 1/9$.\n",
    "For simplicity we will work now with the precision $\\lambda = 1/\\sigma^2 = 9$.\n",
    "According to [1], we can obtaint the posterior of the mean $\\mu$ after $n$ observed samples as\n",
    "\\begin{align}\n",
    "p(\\mu|\\mathcal D, \\lambda) &= \\mathcal N(\\mu|\\mu_n, \\lambda_n) \\\\\n",
    "\\lambda_n &= \\lambda_0 + n\\lambda \\\\\n",
    "\\mu_n &= \\frac{\\bar x n \\lambda + \\mu_0\\lambda_0}{\\lambda_n},\n",
    "\\end{align}\n",
    "where $\\lambda_0$ and $\\mu_0$ is the mean and the precision of the prior distribution over the mean $\\mu$ of the unknown Gaussain, and $\\bar x = 1/n \\sum_i x_i$ is the sample mean.\n",
    "\n",
    "[1] Conjugate Bayesian analysis of the Gaussian distribution, *Kevin P. Murphy*, [PDF](http://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf)\n",
    "\n",
    "### Drawing samples\n",
    "\n",
    "The underlying process is the following:\n",
    "* First, a latent context $c$ is sampled randomly from a uniform distribution in the range $[-5, 5]$.\n",
    "* Then, around that context, we draw a Gaussian sample with $\\mu = c$ and $\\sigma = 0.3$. This is the observation.\n",
    "\n",
    "As training data, we sample $n$ latent contexts $c_i$ and for each context a single sample from the Gaussian distribution $\\mathcal N(c_i,\\sigma)$ at that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 500\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    np.zeros((num_samples,2)),\n",
    "    columns=['context','samples']\n",
    ")\n",
    "\n",
    "data['context'] = np.random.uniform(low=-5, high=5, size=(num_samples,1))\n",
    "\n",
    "noise_mean = 0\n",
    "noise_std = 1/3\n",
    "data['samples'] = np.random.normal(loc=data['context'] + noise_mean, scale=noise_std)\n",
    "\n",
    "# kernel density estimation on the samples\n",
    "ax = data['samples'].plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the methods, we sample a single test context $c^\\ast$ from the same uniform distribution and then draw samples $y_i$ from the Gaussian $\\mathcal N(c^\\ast,\\sigma)$ as observations.\n",
    "From these observations we then want to infer the latent context $c^\\ast$ using Bayes' rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = np.random.uniform(low=-5, high=5, size=1)\n",
    "\n",
    "groundTruthN = stats.norm(loc=test_context + noise_mean, scale=noise_std)\n",
    "\n",
    "\n",
    "# draw samples\n",
    "numTestSamples = 50\n",
    "plotSteps = 1\n",
    "\n",
    "testData = pd.DataFrame(\n",
    "#     np.random.normal(loc=test_context + noise_mean,\n",
    "#                      scale=noise_std,\n",
    "#                      size=(numTestSamples)),\n",
    "    groundTruthN.rvs(size=numTestSamples),\n",
    "    columns=['samples']\n",
    ")\n",
    "\n",
    "# # Ground Truth\n",
    "# print(test_context + noise_mean)\n",
    "# testData['samples'][10] = test_context + noise_std * 4\n",
    "# testData['samples'][40] = test_context - noise_std * 4\n",
    "\n",
    "# # テストデータに外れ値を入れる\n",
    "# print(testData['samples'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical posterior estimation with conjugate prior\n",
    "\n",
    "Although this data is obviously not Gaussian distributed, we can now simply assume a Gaussian prior on the mean of the unknown distribution and compute the mean $\\mu_0$ and the variance $\\sigma_0^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_0 = data['samples'].mean()\n",
    "var_0 = data['samples'].var()\n",
    "lamb_0 = 1/var_0\n",
    "\n",
    "priorN = stats.norm(loc=mu_0, scale=data['samples'].std())\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "ax = data['samples'].plot.kde()\n",
    "\n",
    "ax = ax.plot(x, priorN.pdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the posterior values and plot the posterior distribution for different number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 1/noise_std**2\n",
    "\n",
    "testData['postLambda'] = (np.array(range(numTestSamples)) + 1) * lamb + lamb_0\n",
    "testData['postMu'] = (testData['samples'].cumsum() * lamb + mu_0 * lamb_0) / testData['postLambda']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimate\n",
    "\n",
    "Computing the maximum likelihood estimate of the mean is easy.\n",
    "Even more with the built-in methods of Pandas.\n",
    "By using the expanding() grouping, we obtain the mean of the past to current observations for each observation.\n",
    "With the standard deviation, we can compute the standard deviation of the ML estimator as\n",
    "\\begin{align}\n",
    "    \\sigma_{\\mu_{ML}} = \\frac{\\sigma_{ML}}{\\sqrt{n}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData['mlMu'] = testData['samples'].expanding().mean()\n",
    "testData['mlStd'] = testData['samples'].expanding().std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Bayes' Rule\n",
    "\n",
    "Now we can compute the kernel Bayes rule estimate of the Gaussian distribution.\n",
    "First, we need to choose some hyper-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 100         # number of data points in the kernel matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the bandwiths of the kernel functions according to the median trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bandwidths = {'context': 0, 'samples': 0}\n",
    "\n",
    "# for k in ['context', 'samples']:\n",
    "#     distances = spatial.distance.pdist(data[[k]], metric='sqeuclidean')\n",
    "#     bandwidths[k] = np.sqrt(np.median(distances))\n",
    "\n",
    "# bandwidth_factor_k = np.exp(.0)    # bandwidth factor of the kernel function k\n",
    "# bandwidth_factor_g = np.exp(.0)    # bandwidth factor of the kernel function g\n",
    "\n",
    "# bandwidth_k = bandwidth_factor_k * bandwidths['context']\n",
    "# bandwidth_g = bandwidth_factor_g * bandwidths['samples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a random subset of the training data for learning the kernel Kalman rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_set = data[['context', 'samples']].sample(n=kernel_size)#.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_k = ExponentialQuadraticKernel()\n",
    "# kernel_k.bandwidth = bandwidth_k\n",
    "# kernel_k.normalized = True\n",
    "\n",
    "# K = kernel_k(reference_set[['context']].values)\n",
    "\n",
    "# kernel_g = ExponentialQuadraticKernel()\n",
    "# kernel_g.bandwidth = bandwidth_g\n",
    "\n",
    "# G = kernel_g(reference_set[['samples']].values)\n",
    "# k_g = lambda y: kernel_g(reference_set[['samples']].values, y)\n",
    "\n",
    "# X = reference_set[['samples']].values\n",
    "\n",
    "# f, axs = plt.subplots(1,2)\n",
    "# _= axs[0].imshow(K)\n",
    "# _= axs[1].imshow(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute some matrices before the iterative update of the mean estimate and embed the prior distribution over all training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = linalg.solve(K + alpha1 * np.eye(kernel_size), K, assume_a='pos')\n",
    "\n",
    "# K_all = kernel_k(reference_set[['context']].values,\n",
    "#                                data[['context']].values)\n",
    "# C_0 = linalg.solve(K + alpha1 * np.eye(kernel_size), K_all, assume_a='pos')\n",
    "\n",
    "# m_0 = np.mean(C_0, axis=1, keepdims=True)\n",
    "\n",
    "# _ = plt.plot(m_0, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KBR(a)\n",
    "\n",
    "- bandwidth tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_KBRa = []\n",
    "min_RMSE_KBRa = float('inf')\n",
    "best_kbraMu = []\n",
    "best_bandwidth_k = 0\n",
    "best_bandwidth_g = 0\n",
    "\n",
    "# bandwidth tuning\n",
    "bandwidths_k = 4 ** np.linspace(-2, 1, 30)\n",
    "bandwidths_g = 4 ** np.linspace(-2, 1, 30)\n",
    "for bandwidth_k in bandwidths_k:\n",
    "    kernel_k = ExponentialQuadraticKernel()\n",
    "    kernel_k.bandwidth = bandwidth_k\n",
    "    kernel_k.normalized = True\n",
    "\n",
    "    K = kernel_k(reference_set[['context']].values)\n",
    "    for bandwidth_g in bandwidths_g:\n",
    "        kernel_g = ExponentialQuadraticKernel()\n",
    "        kernel_g.bandwidth = bandwidth_g\n",
    "\n",
    "        G = kernel_g(reference_set[['samples']].values)\n",
    "        k_g = lambda y: kernel_g(reference_set[['samples']].values, y)\n",
    "\n",
    "        X = reference_set[['samples']].values\n",
    "\n",
    "        # regularization parameter for the inverses\n",
    "        alphas1_KBRa = [np.exp(i) for i in range(-12, -11)]\n",
    "        alphas1_labels_KBRa= ['exp({0})'.format(i) for i in range(-12, -11)]\n",
    "        alphas2_KBRa = [np.exp(i) for i in range(5, 6)] \n",
    "        alphas2_labels_KBRa= ['exp({0})'.format(i) for i in range(5, 6)]\n",
    "        \n",
    "        min_RMSE_alpha = float('inf')\n",
    "        best_kbraMu_alpha = []\n",
    "\n",
    "        for alpha1 in alphas1_KBRa:\n",
    "            # 平均値を更新する前に事前計算する\n",
    "            C = linalg.solve(K + alpha1 * np.eye(kernel_size), K, assume_a='pos')\n",
    "\n",
    "            K_all = kernel_k(reference_set[['context']].values,\n",
    "                                           data[['context']].values)\n",
    "            C_0 = linalg.solve(K + alpha1 * np.eye(kernel_size), K_all, assume_a='pos')\n",
    "\n",
    "            m_0 = np.mean(C_0, axis=1, keepdims=True)\n",
    "\n",
    "            for alpha2 in alphas2_KBRa:\n",
    "                m_a = m_0.copy()\n",
    "\n",
    "                for i in range(numTestSamples):\n",
    "                    # embed observation\n",
    "                    y = testData.loc[i, 'samples'].reshape((-1,1))\n",
    "                    g_y = k_g(y)\n",
    "\n",
    "                    # perform Bayes update according to KBR(a)\n",
    "                    L_a = C.dot(np.diag(m_a.flat))\n",
    "                    D_a = np.diag(C.dot(m_a).flat)\n",
    "                    D_aG = D_a.dot(G)\n",
    "\n",
    "                    m_a = L_a.T.dot(linalg.solve(D_aG.dot(D_aG) + alpha2 * np.eye(kernel_size), G)).dot(D_a).dot(g_y)\n",
    "                    #normalization\n",
    "                    m_a = m_a / m_a.sum(axis=0)\n",
    "\n",
    "                    # project into state space\n",
    "                    testData.loc[i, 'kbraMu'] = X.T.dot(C.dot(m_a))[0][0]\n",
    "                    \n",
    "                rmse = mean_squared_error(testData['postMu'], testData['kbraMu'])\n",
    "                if rmse < min_RMSE_alpha:\n",
    "                    min_RMSE_alpha = rmse\n",
    "                    best_kbraMu_alpha = testData['kbraMu'].copy()\n",
    "\n",
    "        rmse = mean_squared_error(testData['postMu'], best_kbraMu_alpha)\n",
    "        RMSE_KBRa.append(rmse)\n",
    "        if rmse < min_RMSE_KBRa:\n",
    "            min_RMSE_KBRa = rmse\n",
    "            best_kbraMu = best_kbraMu_alpha.copy()\n",
    "            best_bandwidth_k = bandwidth_k\n",
    "            best_bandwidth_g = bandwidth_g\n",
    "\n",
    "#  plot RMSE heatmap\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.title(\"KBR(a) RMSE\")\n",
    "df_KBRa = pd.DataFrame(data=np.array(RMSE_KBRa).reshape((len(bandwidths_g), len(bandwidths_k))), index=bandwidths_k, columns=bandwidths_g)\n",
    "s = sns.heatmap(df_KBRa, annot=True, cmap='Blues')\n",
    "s.set(xlabel='bandwidth_g', ylabel='bandwidth_k')\n",
    "\n",
    "# best kbraMu\n",
    "print(\"best_bandwidth_k:\", best_bandwidth_k)\n",
    "print(\"best_bandwidth_g:\", best_bandwidth_g)\n",
    "print(\"min_RMSE_KBRa:\", min_RMSE_KBRa)\n",
    "testData['kbraMu'] = best_kbraMu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- regulation parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set best bandwidth\n",
    "kernel_k = ExponentialQuadraticKernel()\n",
    "kernel_k.bandwidth = best_bandwidth_k\n",
    "kernel_k.normalized = True\n",
    "\n",
    "K = kernel_k(reference_set[['context']].values)\n",
    "\n",
    "kernel_g = ExponentialQuadraticKernel()\n",
    "kernel_g.bandwidth = best_bandwidth_g\n",
    "\n",
    "G = kernel_g(reference_set[['samples']].values)\n",
    "k_g = lambda y: kernel_g(reference_set[['samples']].values, y)\n",
    "\n",
    "X = reference_set[['samples']].values\n",
    "\n",
    "# regularization parameter for the inverses\n",
    "alphas1_KBRa = [np.exp(i) for i in range(-17, -7)]\n",
    "alphas1_labels_KBRa= ['exp({0})'.format(i) for i in range(-17, -7)]\n",
    "alphas2_KBRa = [np.exp(i) for i in range(-1, 9)] \n",
    "alphas2_labels_KBRa= ['exp({0})'.format(i) for i in range(-1, 9)]\n",
    "\n",
    "\n",
    "RMSE_KBRa = []\n",
    "min_RMSE_KBRa = 1e9\n",
    "best_kbraMu = []\n",
    "\n",
    "# Start to calculate time\n",
    "start_time = time.time()\n",
    "for alpha1 in alphas1_KBRa:\n",
    "    # 平均値を更新する前に事前計算する\n",
    "    C = linalg.solve(K + alpha1 * np.eye(kernel_size), K, assume_a='pos')\n",
    "\n",
    "    K_all = kernel_k(reference_set[['context']].values,\n",
    "                                   data[['context']].values)\n",
    "    C_0 = linalg.solve(K + alpha1 * np.eye(kernel_size), K_all, assume_a='pos')\n",
    "\n",
    "    m_0 = np.mean(C_0, axis=1, keepdims=True)\n",
    "\n",
    "    for alpha2 in alphas2_KBRa:\n",
    "        m_a = m_0.copy()\n",
    "\n",
    "        for i in range(numTestSamples):\n",
    "            # embed observation\n",
    "            y = testData.loc[i, 'samples'].reshape((-1,1))\n",
    "            g_y = k_g(y)\n",
    "\n",
    "            # perform Bayes update according to KBR(a)\n",
    "            L_a = C.dot(np.diag(m_a.flat))\n",
    "            D_a = np.diag(C.dot(m_a).flat)\n",
    "            D_aG = D_a.dot(G)\n",
    "\n",
    "            m_a = L_a.T.dot(linalg.solve(D_aG.dot(D_aG) + alpha2 * np.eye(kernel_size), G)).dot(D_a).dot(g_y)\n",
    "            #normalization\n",
    "            m_a = m_a / m_a.sum(axis=0)\n",
    "\n",
    "            # project into state space\n",
    "            testData.loc[i, 'kbraMu'] = X.T.dot(C.dot(m_a))[0][0]\n",
    "        \n",
    "        rmse = mean_squared_error(testData['postMu'], testData['kbraMu'])\n",
    "        RMSE_KBRa.append(rmse)\n",
    "        if rmse < min_RMSE_KBRa:\n",
    "            min_RMSE_KBRa = rmse\n",
    "            best_kbraMu = testData['kbraMu'].copy()\n",
    "print(\"elapsed_time:{0}\".format(time.time() - start_time) + \"[sec]\")\n",
    "#  plot RMSE heatmap\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.title(\"KBR(a) RMSE\")\n",
    "df_KBRa = pd.DataFrame(data=np.array(RMSE_KBRa).reshape((len(alphas1_KBRa), len(alphas2_KBRa))), index=alphas1_labels_KBRa, columns=alphas2_labels_KBRa)\n",
    "s = sns.heatmap(df_KBRa, annot=True, cmap='Blues')\n",
    "s.set(xlabel='alpha2', ylabel='alpha1')\n",
    "\n",
    "# best kbraMu\n",
    "print(\"min_RMSE_KBRa:\",  min_RMSE_KBRa)\n",
    "testData['kbraMu'] = best_kbraMu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KBR(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidths = {'context': 0, 'samples': 0}\n",
    "\n",
    "for k in ['context', 'samples']:\n",
    "    distances = spatial.distance.pdist(data[[k]], metric='sqeuclidean')\n",
    "    bandwidths[k] = np.sqrt(np.median(distances))\n",
    "\n",
    "bandwidth_factor_k = np.exp(.0)    # bandwidth factor of the kernel function k\n",
    "bandwidth_factor_g = np.exp(.0)    # bandwidth factor of the kernel function g\n",
    "\n",
    "bandwidth_k = bandwidth_factor_k * bandwidths['context']\n",
    "bandwidth_g = bandwidth_factor_g * bandwidths['samples']\n",
    "print(bandwidth_k)\n",
    "print(bandwidth_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_k = ExponentialQuadraticKernel()\n",
    "kernel_k.bandwidth = bandwidth_k\n",
    "kernel_k.normalized = True\n",
    "\n",
    "K = kernel_k(reference_set[['context']].values)\n",
    "\n",
    "kernel_g = ExponentialQuadraticKernel()\n",
    "kernel_g.bandwidth = bandwidth_g\n",
    "\n",
    "G = kernel_g(reference_set[['samples']].values)\n",
    "k_g = lambda y: kernel_g(reference_set[['samples']].values, y)\n",
    "\n",
    "X = reference_set[['samples']].values\n",
    "\n",
    "f, axs = plt.subplots(1,2)\n",
    "_= axs[0].imshow(K)\n",
    "_= axs[1].imshow(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KBRb\n",
    "alphas1_KBRb = [np.exp(i) for i in range(-17, -7)]\n",
    "alphas1_labels_KBRb= ['exp({0})'.format(i) for i in range(-17, -7)]\n",
    "alphas2_KBRb = [np.exp(i) for i in range(-7, 3)] \n",
    "alphas2_labels_KBRb= ['exp({0})'.format(i) for i in range(-7, 3)]\n",
    "\n",
    "RMSE_KBRb = []\n",
    "min_RMSE_KBRb= 1e9\n",
    "best_kbrbMu = []\n",
    "\n",
    "start_time = time.time()\n",
    "for alpha1 in alphas1_KBRb:\n",
    "    # 平均値を更新する前に事前計算する\n",
    "    C = linalg.solve(K + alpha1 * np.eye(kernel_size), K, assume_a='pos')\n",
    "\n",
    "    K_all = kernel_k(reference_set[['context']].values,\n",
    "                                   data[['context']].values)\n",
    "    C_0 = linalg.solve(K + alpha1 * np.eye(kernel_size), K_all, assume_a='pos')\n",
    "\n",
    "    m_0 = np.mean(C_0, axis=1, keepdims=True)\n",
    "\n",
    "    for alpha2 in alphas2_KBRb:\n",
    "        m_b = m_0.copy()\n",
    "\n",
    "        for i in range(numTestSamples):\n",
    "            # embed observation\n",
    "            y = testData.loc[i, 'samples'].reshape((-1,1))\n",
    "            g_y = k_g(y)\n",
    "\n",
    "            # perform Bayes update according to KBR(b)\n",
    "            D_b = np.diag(C.dot(m_b).flat)\n",
    "            D_bG = D_b.dot(G)\n",
    "\n",
    "            m_b = D_bG.dot(linalg.solve(D_bG.dot(D_bG) + alpha2 * np.eye(kernel_size), D_b)).dot(g_y)\n",
    "            # normalization\n",
    "            m_b = m_b / m_b.sum(axis=0)\n",
    "\n",
    "            # project into state space\n",
    "            testData.loc[i,'kbrbMu'] = X.T.dot(C.dot(m_b))[0][0]\n",
    "        \n",
    "        rmse = mean_squared_error(testData['postMu'], testData['kbrbMu'])\n",
    "        RMSE_KBRb.append(rmse)\n",
    "        if rmse < min_RMSE_KBRb:\n",
    "            best_kbrbMu = testData['kbrbMu'].copy()\n",
    "            min_RMSE_KBRb = rmse\n",
    "print(\"elapsed_time:{0}\".format(time.time() - start_time) + \"[sec]\")\n",
    "#  plot RMSE heatmap\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.title(\"KBR(b) RMSE\")\n",
    "df_KBRb = pd.DataFrame(data=np.array(RMSE_KBRb).reshape((len(alphas1_KBRb), len(alphas2_KBRb))), index=alphas1_labels_KBRb, columns=alphas2_labels_KBRb)\n",
    "s = sns.heatmap(df_KBRb, annot=True, cmap='Blues')\n",
    "s.set(xlabel='alpha2', ylabel='alpha1')\n",
    "\n",
    "# best kbrbMu\n",
    "print(\"min_RMSE_KBRb:\",  min_RMSE_KBRb)\n",
    "testData['kbrbMu'] = best_kbrbMu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KBR(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KBRc\n",
    "alphas1_KBRc = [np.exp(i) for i in range(-9, 0)]\n",
    "alphas1_labels_KBRc= ['exp({0})'.format(i) for i in range(-9, 0)]\n",
    "alphas2_KBRc = [np.exp(i) for i in range(-12, -3)] \n",
    "alphas2_labels_KBRc= ['exp({0})'.format(i) for i in range(-12, -3)]\n",
    "\n",
    "RMSE_KBRc = []\n",
    "min_RMSE_KBRc= 1e9\n",
    "best_kbrcMu = []\n",
    "\n",
    "start_time = time.time()\n",
    "for alpha1 in alphas1_KBRc:\n",
    "    # 平均値を更新する前に事前計算する\n",
    "    C = linalg.solve(K + alpha1 * np.eye(kernel_size), K, assume_a='pos')\n",
    "\n",
    "    K_all = kernel_k(reference_set[['context']].values,\n",
    "                                   data[['context']].values)\n",
    "    C_0 = linalg.solve(K + alpha1 * np.eye(kernel_size), K_all, assume_a='pos')\n",
    "\n",
    "    m_0 = np.mean(C_0, axis=1, keepdims=True)\n",
    "\n",
    "    for alpha2 in alphas2_KBRc:\n",
    "        m_c = m_0.copy()\n",
    "\n",
    "        for i in range(numTestSamples):\n",
    "            # embed observation\n",
    "            y = testData.loc[i, 'samples'].reshape((-1,1))\n",
    "            g_y = k_g(y)\n",
    "\n",
    "            # perform Bayes update according to KBR(c)\n",
    "            D_c = np.diag(C.dot(m_c).flat)\n",
    "            D_cG = D_c.dot(G)\n",
    "\n",
    "            m_c = linalg.solve(D_cG + alpha2 * np.eye(kernel_size), D_c).dot(g_y)\n",
    "            # normalization\n",
    "            m_c = m_c / m_c.sum(axis=0)\n",
    "\n",
    "            # project into state space\n",
    "            testData.loc[i,'kbrcMu'] = X.T.dot(C.dot(m_c))[0][0]\n",
    "\n",
    "        rmse = mean_squared_error(testData['postMu'], testData['kbrcMu'])\n",
    "        RMSE_KBRc.append(rmse)\n",
    "        if rmse < min_RMSE_KBRc:\n",
    "            best_kbrcMu = testData['kbrcMu'].copy()\n",
    "            min_RMSE_KBRc = rmse\n",
    "print(\"elapsed_time:{0}\".format(time.time() - start_time) + \"[sec]\")\n",
    "#  plot RMSE heatmap\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.title(\"KBR(c) RMSE\")\n",
    "df_KBRc = pd.DataFrame(data=np.array(RMSE_KBRc).reshape((len(alphas1_KBRc), len(alphas2_KBRc))), index=alphas1_labels_KBRc, columns=alphas2_labels_KBRc)\n",
    "s = sns.heatmap(df_KBRc, annot=True, cmap='Blues', vmax=0.5)\n",
    "s.set(xlabel='alpha2', ylabel='alpha1')\n",
    "\n",
    "# best kbrcMu\n",
    "print(\"min_RMSE_KBRc:\",  min_RMSE_KBRc)\n",
    "testData['kbrcMu'] = best_kbrcMu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Subspace Kernel Bayes' Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_kernel_size = num_samples\n",
    "# alpha1 = np.exp(-5)      # regularization parameter for the inverses\n",
    "# alpha2 = np.exp(-5)\n",
    "\n",
    "K_r = kernel_k(data[['context']].values, reference_set[['context']].values)\n",
    "\n",
    "G = kernel_g(data[['context']].values)\n",
    "k_g = lambda y: kernel_g(data[['context']].values, y)\n",
    "\n",
    "f, axs = plt.subplots(1,2)\n",
    "_= axs[0].imshow(K_r)\n",
    "_= axs[1].imshow(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # since we use all available training samples to represent the state, the initial weights\n",
    "# # is just a vector with all 1/n\n",
    "# m_0 = np.ones(large_kernel_size) / large_kernel_size\n",
    "\n",
    "# C = np.linalg.solve(K_r.T.dot(K_r) + alpha1 * np.eye(kernel_size), K_r.T)\n",
    "# E = K_r.T.dot(G).dot(K_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas1_subKBR = [np.exp(i) for i in range(-5, 5)]\n",
    "alphas1_labels_subKBR= ['exp({0})'.format(i) for i in range(-10, 0)]\n",
    "alphas2_subKBR = [np.exp(i) for i in range(-5, 5)] \n",
    "alphas2_labels_subKBR= ['exp({0})'.format(i) for i in range(-10, 0)]\n",
    "\n",
    "RMSE_subKBR = []\n",
    "min_RMSE_subKBR = 1e9\n",
    "best_subkbrMu = []\n",
    "\n",
    "start_time = time.time()\n",
    "for alpha1 in alphas1_subKBR:\n",
    "    # since we use all available training samples to represent the state, the initial weights\n",
    "    # is just a vector with all 1/n\n",
    "    m_0 = np.ones(large_kernel_size) / large_kernel_size\n",
    "\n",
    "    C = np.linalg.solve(K_r.T.dot(K_r) + alpha1 * np.eye(kernel_size), K_r.T)\n",
    "    E = K_r.T.dot(G).dot(K_r)\n",
    "    \n",
    "    for alpha2 in alphas2_subKBR:\n",
    "        m = m_0.copy()\n",
    "\n",
    "        for i in range(numTestSamples):\n",
    "            # embed observation\n",
    "            y = testData.loc[i, 'samples'].reshape((-1,1))\n",
    "            g_y = k_g(y)\n",
    "\n",
    "            # perform Bayes update according to subKBR\n",
    "            L = np.diag(m.flat).dot(C.T)\n",
    "            D = C.dot(L)\n",
    "            DE = D.dot(E)\n",
    "            DKg = D.dot(K_r.T).dot(g_y)\n",
    "\n",
    "            m = L.dot(E).dot(np.linalg.solve(DE.dot(DE) + alpha2 * np.eye(kernel_size), DKg))\n",
    "            #normalization\n",
    "            m = m / m.sum(axis=0)\n",
    "\n",
    "            # project into state space\n",
    "            testData.loc[i, 'subkbrMu'] = X.T.dot(C.dot(m))[0][0]\n",
    "\n",
    "        rmse = mean_squared_error(testData['postMu'], testData['subkbrMu'])\n",
    "        RMSE_subKBR.append(rmse)\n",
    "        if rmse < min_RMSE_subKBR:\n",
    "            best_subkbrMu = testData['subkbrMu'].copy()\n",
    "            min_RMSE_subKBR = rmse\n",
    "print(\"elapsed_time:{0}\".format(time.time() - start_time) + \"[sec]\")         \n",
    "#  plot RMSE heatmap\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.title(\"subKBR RMSE\")\n",
    "df_subKBR = pd.DataFrame(data=np.array(RMSE_subKBR).reshape((len(alphas1_subKBR), len(alphas2_subKBR))), index=alphas1_labels_subKBR, columns=alphas2_labels_subKBR)\n",
    "s = sns.heatmap(df_subKBR, annot=True, cmap='Blues')\n",
    "s.set(xlabel='alpha2', ylabel='alpha1')\n",
    "\n",
    "# best subKBRMu\n",
    "print(\"min_RMSE_subKBR:\", min_RMSE_subKBR)\n",
    "testData['subkbrMu'] = best_subkbrMu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Kalman Rule\n",
    "\n",
    "For the kernel Kalman rule, we start again with choosing some hyper-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 100         # number of data points in the kernel matrices\n",
    "# alphaO = np.exp(-10)      # regularization parameter for the inverses\n",
    "# alphaQ = np.exp(-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the bandwidth according to the median trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidths = {'context': 0, 'samples': 0}\n",
    "\n",
    "for k in ['context', 'samples']:\n",
    "    distances = spatial.distance.pdist(data[[k]], metric='sqeuclidean')\n",
    "    bandwidths[k] = np.sqrt(np.median(distances))\n",
    "\n",
    "bandwidth_factor_k = np.exp(.0)    # bandwidth factor of the kernel function k\n",
    "bandwidth_factor_g = np.exp(.0)    # bandwidth factor of the kernel function g\n",
    "\n",
    "bandwidth_k = bandwidth_factor_k * bandwidths['context']\n",
    "bandwidth_g = bandwidth_factor_g * bandwidths['samples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a random subset of the training data for learning the kernel Kalman rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_set = data[['context', 'samples']].sample(n=kernel_size)#.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_k = ExponentialQuadraticKernel()\n",
    "kernel_k.bandwidth = bandwidth_k\n",
    "kernel_k.normalized = True\n",
    "\n",
    "K = kernel_k(reference_set[['context']].values)\n",
    "\n",
    "kernel_g = ExponentialQuadraticKernel()\n",
    "kernel_g.bandwidth = bandwidth_g\n",
    "\n",
    "G = kernel_g(reference_set[['samples']].values)\n",
    "k_g = lambda y: kernel_g(reference_set[['samples']].values, y)\n",
    "\n",
    "f, axs = plt.subplots(1,2)\n",
    "_= axs[0].imshow(K)\n",
    "_= axs[1].imshow(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the conditional operator from the context Hilbert space to the observation Hilbert space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O = linalg.solve(K + alphaO * np.eye(kernel_size), K, assume_a='pos')\n",
    "# X = reference_set[['samples']].values\n",
    "\n",
    "# _r = O - np.eye(kernel_size)\n",
    "# R = (_r.dot(_r.T)) / kernel_size\n",
    "\n",
    "# f, axs = plt.subplots(1,2, sharey='all')\n",
    "# f.set_figwidth(2*f.get_figheight())\n",
    "# _= axs[0].imshow(O)\n",
    "# _= axs[1].imshow(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training data we can now embed the prior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K_all = kernel_k(reference_set[['context']].values,\n",
    "#                                data[['context']].values)\n",
    "# C_0 = linalg.solve(K + alphaO * np.eye(kernel_size), K_all, assume_a='pos')\n",
    "\n",
    "# m_0 = np.mean(C_0, axis=1, keepdims=True)\n",
    "# S_0 = np.cov(C_0)\n",
    "\n",
    "# # normalization\n",
    "# S_0 = 0.5 * (S_0 + S_0.T)\n",
    "# [eig_v, eig_r] = linalg.eigh(S_0)\n",
    "# eig_v[eig_v < 1e-16*eig_v.max()] = 1e-16*eig_v.max()\n",
    "# S_0 = eig_r.dot(np.diag(eig_v)).dot(eig_r.T)\n",
    "\n",
    "# # plotting\n",
    "# f, axs = plt.subplots(1,2)\n",
    "# f.set_figwidth(2*f.get_figheight())\n",
    "# _ = axs[0].plot(m_0, '.')\n",
    "# _ = axs[1].imshow(S_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning kernel's bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_KKR = []\n",
    "min_RMSE_KKR= 1e9\n",
    "best_kkrMu = []\n",
    "best_kkrVar = []\n",
    "best_bandwidth_k = 0\n",
    "best_bandwidth_g = 0\n",
    "\n",
    "start_time = time.time()\n",
    "# bandwidth tuning\n",
    "bandwidths_k = 4 ** np.linspace(-2, 1, 30)\n",
    "bandwidths_g = 4 ** np.linspace(-2, 1, 30)\n",
    "for bandwidth_k in bandwidths_k:\n",
    "    kernel_k = ExponentialQuadraticKernel()\n",
    "    kernel_k.bandwidth = bandwidth_k\n",
    "    kernel_k.normalized = True\n",
    "\n",
    "    K = kernel_k(reference_set[['context']].values)\n",
    "\n",
    "    for bandwidth_g in bandwidths_g:\n",
    "        kernel_g = ExponentialQuadraticKernel()\n",
    "        kernel_g.bandwidth = bandwidth_g\n",
    "\n",
    "        G = kernel_g(reference_set[['samples']].values)\n",
    "        k_g = lambda y: kernel_g(reference_set[['samples']].values, y)\n",
    "\n",
    "        X = reference_set[['samples']].values\n",
    "\n",
    "        alphasO = [np.exp(i) for i in range(-10, -9)]\n",
    "        alphasO_labels= ['exp({0})'.format(i) for i in range(-10, -9)]\n",
    "        alphasQ = [np.exp(i) for i in range(-11, -10)] \n",
    "        alphasQ_labels= ['exp({0})'.format(i) for i in range(-11, -10)]\n",
    "        \n",
    "        min_RMSE_KKR_alpha = float('inf')\n",
    "        best_kkrMu_alpha = []\n",
    "        best_kkrVar_alpha = []\n",
    "\n",
    "        for alphaO in alphasO:\n",
    "            # Compute the conditional operator from the context Hilbert space to the observation Hilbert space\n",
    "            O = linalg.solve(K + alphaO * np.eye(kernel_size), K, assume_a='pos')\n",
    "            X = reference_set[['samples']].values\n",
    "\n",
    "            _r = O - np.eye(kernel_size)\n",
    "            R = (_r.dot(_r.T)) / kernel_size\n",
    "\n",
    "            # From the training data we can now embed the prior distribution\n",
    "            K_all = kernel_k(reference_set[['context']].values,\n",
    "                                           data[['context']].values)\n",
    "            C_0 = linalg.solve(K + alphaO * np.eye(kernel_size), K_all, assume_a='pos')\n",
    "\n",
    "            m_0 = np.mean(C_0, axis=1, keepdims=True)\n",
    "            S_0 = np.cov(C_0)\n",
    "\n",
    "            # normalization\n",
    "            S_0 = 0.5 * (S_0 + S_0.T)\n",
    "            [eig_v, eig_r] = linalg.eigh(S_0)\n",
    "            eig_v[eig_v < 1e-16*eig_v.max()] = 1e-16*eig_v.max()\n",
    "            S_0 = eig_r.dot(np.diag(eig_v)).dot(eig_r.T)\n",
    "\n",
    "            for alphaQ in alphasQ:\n",
    "                # initialize belief states\n",
    "                m = m_0.copy()\n",
    "                S = S_0.copy()\n",
    "\n",
    "                for i in range(numTestSamples):\n",
    "                    # embed observation\n",
    "                    y = testData.loc[i, 'samples'].reshape((-1,1))\n",
    "                    g_y = k_g(y)\n",
    "\n",
    "                    # compute kernel Kalman gain\n",
    "                    Q_nominator_T = O.dot(S.T)\n",
    "                    Q_denominator_T = (O.dot(S).dot(O.T) + R).dot(G) + alphaQ * np.eye(kernel_size)\n",
    "                #     Q_denominator_T = O.dot(S).dot(O.T).dot(G) + alphaQ * np.eye(kernel_size)\n",
    "                    Q = linalg.solve(Q_denominator_T, Q_nominator_T, overwrite_a=True, overwrite_b=True).T\n",
    "\n",
    "                    # perform Kalman update\n",
    "                    m = m + Q.dot(g_y - G.dot(O).dot(m))\n",
    "                    S = S - Q.dot(G).dot(O).dot(S)\n",
    "\n",
    "                    # normalization\n",
    "                    S = 0.5 * (S + S.T)\n",
    "                    [eig_v, eig_r] = linalg.eigh(S)\n",
    "                    eig_v[eig_v < 1e-16*eig_v.max()] = 1e-16*eig_v.max()\n",
    "                    S = eig_r.dot(np.diag(eig_v)).dot(eig_r.T)\n",
    "\n",
    "                    m = m / m.sum(axis=0)\n",
    "\n",
    "                    # project into state space\n",
    "                    testData.loc[i,'kkrMu'] = X.T.dot(O.dot(m))[0][0]\n",
    "                    testData.loc[i,'kkrVar'] = X.T.dot(O.dot(S).dot(O.T)).dot(X)[0][0]\n",
    "                #     testData.loc[i,'kkrMAP'] =\n",
    "                rmse = mean_squared_error(testData['postMu'], testData['kkrMu'])\n",
    "                if rmse < min_RMSE_KKR_alpha:\n",
    "                    min_RMSE_KKR_alpha = rmse\n",
    "                    best_kkrMu_alpha = testData['kkrMu'].copy()\n",
    "                    best_kkrVar_alpha = testData['kkrVar'].copy()\n",
    "        RMSE_KKR.append(min_RMSE_KKR_alpha)\n",
    "        if min_RMSE_KKR_alpha < min_RMSE_KKR:\n",
    "            best_kkrMu = best_kkrMu_alpha.copy()\n",
    "            best_kkrVar = best_kkrVar_alpha.copy()\n",
    "            min_RMSE_KKR = min_RMSE_KKR_alpha\n",
    "            best_bandwidth_k = bandwidth_k\n",
    "            best_bandwidth_g = bandwidth_g\n",
    "print(\"elapsed_time:{0}\".format(time.time() - start_time) + \"[sec]\")\n",
    "#  plot RMSE heatmap\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.title(\"KKR RMSE\")\n",
    "df_KKR = pd.DataFrame(data=np.array(RMSE_KKR).reshape((len(bandwidths_g), len(bandwidths_k))), index=bandwidths_k, columns=bandwidths_g)\n",
    "s = sns.heatmap(df_KKR, annot=True, cmap='Blues')\n",
    "s.set(xlabel='bandwidth_g', ylabel='bandwidth_k')\n",
    "\n",
    "# best kkrMu\n",
    "print(\"best_bandwidth_k:\", best_bandwidth_k)\n",
    "print(\"best_bandwidth_g:\", best_bandwidth_g)\n",
    "print(\"min_RMSE_KKR:\", min_RMSE_KKR)\n",
    "testData['kkrMu'] = best_kkrMu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can now iteratively apply the kernel Kalman rule to get the posterior estimates of the mean for each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphasO = [np.exp(i) for i in range(-15, -5)]\n",
    "alphasO_labels= ['exp({0})'.format(i) for i in range(-15, -5)]\n",
    "alphasQ = [np.exp(i) for i in range(-15, -5)] \n",
    "alphasQ_labels= ['exp({0})'.format(i) for i in range(-15, -5)]\n",
    "\n",
    "RMSE_KKR = []\n",
    "min_RMSE_KKR= 1e9\n",
    "best_kkrMu = []\n",
    "best_kkrVar = []\n",
    "\n",
    "# Start to calculate time\n",
    "start_time = time.time()\n",
    "for alphaO in alphasO:\n",
    "    # Compute the conditional operator from the context Hilbert space to the observation Hilbert space\n",
    "    O = linalg.solve(K + alphaO * np.eye(kernel_size), K, assume_a='pos')\n",
    "    X = reference_set[['samples']].values\n",
    "\n",
    "    _r = O - np.eye(kernel_size)\n",
    "    R = (_r.dot(_r.T)) / kernel_size\n",
    "\n",
    "    # From the training data we can now embed the prior distribution\n",
    "    K_all = kernel_k(reference_set[['context']].values,\n",
    "                                   data[['context']].values)\n",
    "    C_0 = linalg.solve(K + alphaO * np.eye(kernel_size), K_all, assume_a='pos')\n",
    "\n",
    "    m_0 = np.mean(C_0, axis=1, keepdims=True)\n",
    "    S_0 = np.cov(C_0)\n",
    "\n",
    "    # normalization\n",
    "    S_0 = 0.5 * (S_0 + S_0.T)\n",
    "    [eig_v, eig_r] = linalg.eigh(S_0)\n",
    "    eig_v[eig_v < 1e-16*eig_v.max()] = 1e-16*eig_v.max()\n",
    "    S_0 = eig_r.dot(np.diag(eig_v)).dot(eig_r.T)\n",
    "\n",
    "    for alphaQ in alphasQ:\n",
    "        # initialize belief states\n",
    "        m = m_0.copy()\n",
    "        S = S_0.copy()\n",
    "\n",
    "        for i in range(numTestSamples):\n",
    "            # embed observation\n",
    "            y = testData.loc[i, 'samples'].reshape((-1,1))\n",
    "            g_y = k_g(y)\n",
    "\n",
    "            # compute kernel Kalman gain\n",
    "            Q_nominator_T = O.dot(S.T)\n",
    "            Q_denominator_T = (O.dot(S).dot(O.T) + R).dot(G) + alphaQ * np.eye(kernel_size)\n",
    "        #     Q_denominator_T = O.dot(S).dot(O.T).dot(G) + alphaQ * np.eye(kernel_size)\n",
    "            Q = linalg.solve(Q_denominator_T, Q_nominator_T, overwrite_a=True, overwrite_b=True).T\n",
    "\n",
    "            # perform Kalman update\n",
    "            m = m + Q.dot(g_y - G.dot(O).dot(m))\n",
    "            S = S - Q.dot(G).dot(O).dot(S)\n",
    "\n",
    "            # normalization\n",
    "            S = 0.5 * (S + S.T)\n",
    "            [eig_v, eig_r] = linalg.eigh(S)\n",
    "            eig_v[eig_v < 1e-16*eig_v.max()] = 1e-16*eig_v.max()\n",
    "            S = eig_r.dot(np.diag(eig_v)).dot(eig_r.T)\n",
    "\n",
    "            m = m / m.sum(axis=0)\n",
    "\n",
    "            # project into state space\n",
    "            testData.loc[i,'kkrMu'] = X.T.dot(O.dot(m))[0][0]\n",
    "            testData.loc[i,'kkrVar'] = X.T.dot(O.dot(S).dot(O.T)).dot(X)[0][0]\n",
    "        #     testData.loc[i,'kkrMAP'] =\n",
    "        rmse = mean_squared_error(testData['postMu'], testData['kkrMu'])\n",
    "        RMSE_KKR.append(rmse)\n",
    "        if rmse < min_RMSE_KKR:\n",
    "            best_kkrMu = testData['kkrMu'].copy()\n",
    "            best_kkrVar = testData['kkrVar'].copy()\n",
    "            min_RMSE_KKR = rmse\n",
    "print(\"elapsed_time:{0}\".format(time.time() - start_time) + \"[sec]\")\n",
    "#  plot RMSE heatmap\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.title(\"KKR RMSE\")\n",
    "df_KKR = pd.DataFrame(data=np.array(RMSE_KKR).reshape((len(alphasO), len(alphasQ))), index=alphasO_labels, columns=alphasQ_labels)\n",
    "s = sns.heatmap(df_KKR, annot=True, cmap='Blues')\n",
    "s.set(xlabel='alphaQ', ylabel='alphaO')\n",
    "\n",
    "# best kkrMu\n",
    "print(\"min_RMSE_KKR:\", min_RMSE_KKR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE_myKKR = []\n",
    "# min_RMSE_myKKR = 1e9\n",
    "# best_mykkrMu = []\n",
    "# best_alpha_a = 0\n",
    "\n",
    "# # regularization parameter\n",
    "# alphas_a = [np.exp(i) for i in range(-20, 0)]\n",
    "# alphas_a_labels= ['exp({0})'.format(i) for i in range(-20, 0)]\n",
    "\n",
    "# for alpha_a in alphas_a:\n",
    "#     A = linalg.inv(K + alpha_a * np.eye(kernel_size))\n",
    "#     m = linalg.solve(A.dot(G).dot(A).dot(K), A.dot(kernel_g(reference_set[['samples']].values, testData[['samples']].values)), overwrite_a=True, overwrite_b=True)\n",
    "\n",
    "#     #normalization\n",
    "#     m = m / m.sum(axis=0)\n",
    "    \n",
    "#     X = reference_set[['samples']].values\n",
    "#     testData['mykkrMu'] = X.T.dot(A.dot(K).dot(m))[0]\n",
    "    \n",
    "#     rmse = mean_squared_error(testData['postMu'], testData['mykkrMu'])\n",
    "#     RMSE_myKKR.append(rmse)\n",
    "#     if rmse < min_RMSE_myKKR:\n",
    "#         best_mykkrMu = testData['mykkrMu'].copy()\n",
    "#         min_RMSE_myKKR = rmse\n",
    "#         best_alpha_a = alpha_a\n",
    "\n",
    "# plt.figure(figsize=(20,15))\n",
    "# plt.title(\"myKKR RMSE\")\n",
    "# df_KKR = pd.DataFrame(data=np.array(RMSE_myKKR).reshape((len(alphas_a), 1)), index=alphas_a_labels)\n",
    "# s = sns.heatmap(df_KKR, annot=True, cmap='Blues', vmax=1)\n",
    "# s.set(ylabel='alpha_a')\n",
    "# print(\"best_alpha_a\", best_alpha_a)\n",
    "# testData['mykkrMu'] = best_mykkrMu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_a = np.exp(-11)\n",
    "# A = linalg.inv(K + alpha_a * np.eye(kernel_size))\n",
    "# m = (A.dot(kernel_g(reference_set[['samples']].values, testData[['samples']].values)))/ numTestSamples\n",
    "\n",
    "# #normalization\n",
    "# m = m / m.sum(axis=0)\n",
    "\n",
    "# print(X.T.dot(A.dot(K).dot(m))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Subspace Kernel Kalman Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_s = kernel_k(data[['context']].values, reference_set[['context']].values)\n",
    "\n",
    "G = kernel_g(data[['samples']].values)\n",
    "k_g = lambda y: kernel_g(data[['samples']].values, y)\n",
    "\n",
    "f, axs = plt.subplots(1,2)\n",
    "_= axs[0].imshow(K_s)\n",
    "_= axs[1].imshow(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphasO = [np.exp(i) for i in range(-15, -5)]\n",
    "alphasO_labels= ['exp({0})'.format(i) for i in range(-15, -5)]\n",
    "alphasQ = [np.exp(i) for i in range(-10, 0)] \n",
    "alphasQ_labels= ['exp({0})'.format(i) for i in range(-10, 0)]\n",
    "\n",
    "RMSE_subKKR = []\n",
    "min_RMSE_subKKR= 1e9\n",
    "best_subkkrMu = []\n",
    "\n",
    "for alphaO in alphasO:\n",
    "    # Compute the conditional operator from the context Hilbert space to the observation Hilbert space\n",
    "    O = linalg.inv((K_s.T).dot(K_s) + alphaO * np.eye(kernel_size))\n",
    "    X = data[['samples']].values\n",
    "\n",
    "    _r = O - np.eye(kernel_size)\n",
    "    R = (_r.dot(_r.T)) / kernel_size\n",
    "\n",
    "    # From the training data we can now embed the prior distribution\n",
    "    K_s_all = kernel_k(reference_set[['context']].values, data[['context']].values)\n",
    "#     C_0 = linalg.solve(K + alphaO * np.eye(kernel_size), K_all, assume_a='pos')\n",
    "\n",
    "    m_0 = np.mean(K_s_all, axis=1, keepdims=True)\n",
    "    S_0 = np.cov(K_s_all)\n",
    "\n",
    "    # normalization\n",
    "    S_0 = 0.5 * (S_0 + S_0.T)\n",
    "    [eig_v, eig_r] = linalg.eigh(S_0)\n",
    "    eig_v[eig_v < 1e-16*eig_v.max()] = 1e-16*eig_v.max()\n",
    "    S_0 = eig_r.dot(np.diag(eig_v)).dot(eig_r.T)\n",
    "\n",
    "    for alphaQ in alphasQ:\n",
    "        # initialize belief states\n",
    "        m = m_0.copy()\n",
    "        S = S_0.copy()\n",
    "\n",
    "        for i in range(numTestSamples):\n",
    "            # embed observation\n",
    "            y = testData.loc[i, 'samples'].reshape((-1,1))\n",
    "            g_y = k_g(y)\n",
    "\n",
    "            # compute kernel Kalman gain\n",
    "            Q_dom = linalg.inv((K_s.T).dot(G).dot(K_s).dot(O).dot(S).dot(O.T) + alphaQ * np.eye(kernel_size))\n",
    "            Q = S.dot(O.T).dot(Q_dom).dot(K_s_all)\n",
    "\n",
    "            # perform Kalman update\n",
    "            m = m + Q.dot(g_y - G.dot(K_s).dot(O).dot(m))\n",
    "            S = S - Q.dot(G).dot(K_s).dot(O).dot(S)\n",
    "\n",
    "            # normalization\n",
    "            S = 0.5 * (S + S.T)\n",
    "            [eig_v, eig_r] = linalg.eigh(S)\n",
    "            eig_v[eig_v < 1e-16*eig_v.max()] = 1e-16*eig_v.max()\n",
    "            S = eig_r.dot(np.diag(eig_v)).dot(eig_r.T)\n",
    "\n",
    "            m = m / m.sum(axis=0)\n",
    "\n",
    "            # project into state space\n",
    "            testData.loc[i,'subkkrMu'] = X.T.dot(K_s).dot(O).dot(m)[0][0]\n",
    "            testData.loc[i,'subkkrVar'] = X.T.dot(K_s.dot(O).dot(S).dot(O).dot(K_s.T)).dot(X)[0][0]\n",
    "        #     testData.loc[i,'kkrMAP'] =\n",
    "        rmse = mean_squared_error(testData['postMu'], testData['subkkrMu'])\n",
    "        RMSE_subKKR.append(rmse)\n",
    "        if rmse < min_RMSE_subKKR:\n",
    "            best_subkkrMu = testData['subkkrMu'].copy()\n",
    "            min_RMSE_subKKR = rmse\n",
    "\n",
    "#  plot RMSE heatmap\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.title(\"subKKR RMSE\")\n",
    "df_subKKR = pd.DataFrame(data=np.array(RMSE_subKKR).reshape((len(alphasO), len(alphasQ))), index=alphasO_labels, columns=alphasQ_labels)\n",
    "s = sns.heatmap(df_subKKR, annot=True, cmap='Blues')\n",
    "s.set(xlabel='alphaQ', ylabel='alphaO')\n",
    "\n",
    "# best kkrMu\n",
    "print(\"min_RMSE_subKKR:\", min_RMSE_subKKR)\n",
    "testData['subkkrMu'] = best_subkkrMu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Posterior Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "plotXMin = groundTruthN.ppf(.001)[0]\n",
    "# plotXMin = -0.7\n",
    "plotXMax = groundTruthN.ppf(.999)[0]\n",
    "# plotXMax = 0.7\n",
    "\n",
    "x = np.arange(plotXMin,plotXMax,0.01)\n",
    "\n",
    "p = figure(title=\"estimation of mean\",\n",
    "           plot_height=400, plot_width=800, x_range=(plotXMin,plotXMax), y_range=(0,10))\n",
    "\n",
    "# add ground truth\n",
    "gt_ray = p.ray(test_context, 0,\n",
    "               length=1, angle=np.pi/2,\n",
    "               line_width=2, line_color=\"black\",\n",
    "               legend=\"ground-truth\")\n",
    "post_line = p.line(x,\n",
    "                   stats.norm.pdf(x,\n",
    "                            loc=testData.loc[numTestSamples-1, 'postMu'],\n",
    "                            scale=np.sqrt(1/testData.loc[0, 'postLambda'])),\n",
    "                   line_width=2,  line_dash=\"dashed\", line_color=\"black\",\n",
    "                   legend=\"posterior\")\n",
    "map_line = p.line([testData.loc[numTestSamples-1, 'postMu']]*2, [0, 10],\n",
    "                  line_width=2,  line_dash=\"dashed\", line_color=\"black\",\n",
    "                  legend=\"maximum a-posteriori\")\n",
    "ml_line = p.line([testData.loc[numTestSamples-1, 'mlMu']]*2, [0, 10],\n",
    "                 line_width=2, line_color=\"#672E3B\",\n",
    "                 legend=\"maximum likelihood\")\n",
    "kkr_fit_line = p.line(x,\n",
    "                      stats.norm.pdf(x,\n",
    "                                     loc=testData.loc[numTestSamples-1, 'kkrMu'],\n",
    "                                     scale=np.sqrt(testData.loc[0, 'kkrVar'])),\n",
    "                      line_width=2, line_color=\"red\",\n",
    "                      legend=\"KKR (RBF kernel)\")\n",
    "kkr_exp_line = p.line([testData.loc[numTestSamples-1, 'kkrMu']]*2, [0, 10],\n",
    "                 line_width=2, line_color=\"red\",\n",
    "                 legend=\"KKR (RBF kernel) expectation\")\n",
    "subkkr_exp_line = p.line([testData.loc[numTestSamples-1, 'subkkrMu']]*2, [0, 10],\n",
    "                 line_width=2, line_color=\"pink\",\n",
    "                 legend=\"subKKR (RBF kernel) expectation\")\n",
    "# mykkr_exp_line = p.line([testData.loc[numTestSamples-1, 'mykkrMu']]*2, [0, 10],\n",
    "#                  line_width=2, line_color=\"orange\",\n",
    "#                  legend=\"myKKR (RBF kernel) expectation\")\n",
    "# kkrRQ_exp_line = p.line([testData.loc[numTestSamples-1, 'kkrRQMu']]*2, [0, 10],\n",
    "#                  line_width=2, line_color=\"orange\",\n",
    "#                  legend=\"KKR (RQ kernel)\")\n",
    "kbra_exp_line = p.line([testData.loc[numTestSamples-1, 'kbraMu']]*2, [0, 10],\n",
    "                 line_width=2, line_color=\"#4F84C4\", line_dash=\"solid\",\n",
    "                 legend=\"KBR(a) expectation\")\n",
    "kbrb_exp_line = p.line([testData.loc[numTestSamples-1, 'kbrbMu']]*2, [0, 10],\n",
    "                 line_width=2, line_color=\"#4F84C4\", line_dash=\"dashed\",\n",
    "                 legend=\"KBR(b) expectation\")\n",
    "kbrc_exp_line = p.line([testData.loc[numTestSamples-1, 'kbrcMu']]*2, [0, 10],\n",
    "                 line_width=2, line_color=\"#4F84C4\", line_dash=\"dotted\",\n",
    "                 legend=\"KBR(c) expectation\")\n",
    "subkbrc_exp_line = p.line([testData.loc[numTestSamples-1, 'subkbrMu']]*2, [0, 10],\n",
    "                 line_width=2, line_color=\"#844FC4\", line_dash=\"solid\",\n",
    "                 legend=\"subKBR expectation\")\n",
    "\n",
    "\n",
    "samples_scatter = p.scatter(testData.loc[numTestSamples-1:numTestSamples-1, 'samples'],0.2, color=\"orange\", size=5)\n",
    "\n",
    "h = show(p, notebook_handle=True)\n",
    "\n",
    "@interact(n=(1,numTestSamples,plotSteps))\n",
    "def update_posterior_plot(n=1):\n",
    "    _d = testData.loc[n-1]\n",
    "    post_line.data_source.data['y'] = stats.norm.pdf(x,\n",
    "                                             loc=_d.loc['postMu'],\n",
    "                                             scale=np.sqrt(1/_d.loc['postLambda']))\n",
    "    map_line.data_source.data['x'] = 2*[_d.loc['postMu']]\n",
    "    ml_line.data_source.data['x'] = 2*[_d.loc['mlMu']]\n",
    "    \n",
    "    kkr_fit_line.data_source.data['y'] = stats.norm.pdf(x,\n",
    "                                                loc=_d.loc['kkrMu'],\n",
    "                                                scale=np.sqrt(_d.loc['kkrVar']))\n",
    "    kkr_exp_line.data_source.data['x'] = 2*[_d.loc['kkrMu']]\n",
    "    kbra_exp_line.data_source.data['x'] = 2*[_d.loc['kbraMu']]\n",
    "    kbrb_exp_line.data_source.data['x'] = 2*[_d.loc['kbrbMu']]\n",
    "    kbrc_exp_line.data_source.data['x'] = 2*[_d.loc['kbrcMu']]\n",
    "    subkbrc_exp_line.data_source.data['x'] = 2*[_d.loc['subkbrMu']]\n",
    "    \n",
    "    samples_scatter.data_source.data['x'] = testData.loc[0:n-1, 'samples'].values\n",
    "    push_notebook(handle=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the predictive distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(title=\"predictive distributions\",\n",
    "           plot_height=400, plot_width=800, x_range=(plotXMin,plotXMax), y_range=(0,10))\n",
    "\n",
    "# add ground truth\n",
    "gt_line = p.line(x, \n",
    "               groundTruthN.pdf(x),\n",
    "               line_width=2, line_dash=\"dashed\", line_color=\"gray\",\n",
    "               legend=\"ground truth\")\n",
    "post_line = p.line(x,\n",
    "                   stats.norm.pdf(x,\n",
    "                            loc=testData.loc[0, 'postMu'],\n",
    "                            scale=np.sqrt(1/testData.loc[0, 'postLambda'] + noise_std**2)),\n",
    "                   line_color=\"blue\",\n",
    "                   legend=\"posterior\"\n",
    "                  )\n",
    "ml_line = p.line(x,\n",
    "                 stats.norm.pdf(x,\n",
    "                          loc=testData.loc[1, 'mlMu'],\n",
    "                          scale=noise_std),\n",
    "                 line_color=\"red\",\n",
    "                 legend=\"maximum likelihood\"\n",
    "                )\n",
    "samples_scatter = p.scatter(testData.loc[0:0, 'samples'],0.2, color=\"orange\")\n",
    "\n",
    "h_predictive = show(p, notebook_handle=True)\n",
    "\n",
    "@interact(n=(1,numTestSamples,plotSteps))\n",
    "def update_predictive_plot(n=1):\n",
    "    post_line.data_source.data['y'] = stats.norm.pdf(x,\n",
    "                                               loc=testData.loc[n-1, 'postMu'],\n",
    "                                               scale=np.sqrt(1/testData.loc[n-1, 'postLambda'] + noise_std**2))\n",
    "    ml_line.data_source.data['y'] = stats.norm.pdf(x,\n",
    "                                             loc=testData.loc[n-1, 'mlMu'],\n",
    "                                             scale=noise_std)\n",
    "    samples_scatter.data_source.data['x'] = testData.loc[0:n-1, 'samples'].values\n",
    "    push_notebook(handle=h_predictive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error = []\n",
    "error.append(abs(testData['kbraMu'] - testData['postMu']).values)\n",
    "error.append(abs(testData['kbrbMu'] - testData['postMu']).values)\n",
    "error.append(abs(testData['kbrcMu'] - testData['postMu']).values)\n",
    "error.append(abs(testData['subkbrMu'] - testData['postMu']).values)\n",
    "error.append(abs(testData['kkrMu'] - testData['postMu']).values)\n",
    "error.append(abs(testData['subkkrMu'] - testData['postMu']).values)\n",
    "# error.append(abs(testData['mykkrMu'] - testData['postMu']).values)\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "kbraMuPlt = plt.plot([i for i in range(numTestSamples)], error[0], color=\"blue\", linestyle=\"solid\")\n",
    "kbrbMuPlt = plt.plot([i for i in range(numTestSamples)], error[1], color=\"blue\", linestyle=\"dashed\")\n",
    "kbrcMuPlt = plt.plot([i for i in range(numTestSamples)], error[2], color=\"blue\", linestyle=\"dotted\")\n",
    "subkbrMuPlt = plt.plot([i for i in range(numTestSamples)], error[3], color=\"deepskyblue\", linestyle=\"solid\")\n",
    "kkrMuPlt = plt.plot([i for i in range(numTestSamples)], error[4], color=\"red\")\n",
    "subkkrMuPlt = plt.plot([i for i in range(numTestSamples)], error[5], color=\"pink\")\n",
    "# mykkrMuPlt = plt.plot([i for i in range(numTestSamples)], error[6], color=\"orange\")\n",
    "plt.xlabel('sample num', fontsize=\"15\")\n",
    "plt.ylabel('RMSE',  fontsize=\"15\")\n",
    "plt.legend((kbraMuPlt[0], kbrbMuPlt[0], kbrcMuPlt[0],  subkbrMuPlt[0], kkrMuPlt[0], subkkrMuPlt[0]), (\"kbraMu\", \"kbrbMu\", \"kbrcMu\", \" subkbrMu\", \"kkrMu\", \"subkkrMu\"), loc=2, fontsize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_elapsed_time:7.390975952148438e-05[sec]\n",
      "[0, 1, 3, 6, 10]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def sumall(value):\n",
    "    return sum(range(1, value + 1))\n",
    "\n",
    "# Order\n",
    "answer = []\n",
    "start_time = time.time()\n",
    "for i in range(0, 5):\n",
    "    answer.append(sumall(i))\n",
    "print(\"order_elapsed_time:{0}\".format(time.time() - start_time) + \"[sec]\")\n",
    "\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
